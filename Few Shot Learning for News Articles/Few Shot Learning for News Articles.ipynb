{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381fc209-67b5-469c-bcbe-6bdb8886b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vijay Venkatesan\n",
    "# CS 6120 (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "41aa9c73-5e5e-4fcf-847c-ec990876fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4798d2-503d-4d1a-908e-84dcf6054066",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f488240-6982-44a8-a6dd-1caa13745890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in each of the csv files and store it in a dataframe\n",
    "# Add a column to each dataframe called \"RealNews?\"\n",
    "# Concatenate both dataframes into a new dataframe called df\n",
    "\n",
    "df_real = pd.read_csv('True.csv')\n",
    "df_real['RealNews?'] = True\n",
    "df_fake = pd.read_csv('Fake.csv')\n",
    "df_fake['RealNews?'] = False\n",
    "df = pd.concat([df_real, df_fake], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a1fdc26-849a-45bb-8735-57588e121a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a new column called 'document' which combines the content in the 'title' and 'text' columns\n",
    "\n",
    "df['document'] = df[['title', 'text']].agg(' '.join, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd329f6-daf0-45da-a370-e14a3b3d59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the data in the 'document' column by lowercasing the text which helps reduce the feature space\n",
    "\n",
    "df['document'] = df['document'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5424be5-89a2-4d6c-b365-af61a68e49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains examples of both fake news/real news where the label associated with each example indicates whether it is real or fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78179524-a0da-4380-8721-aba711467077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, test, and validation sets using an 80/10/10 split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_validation, df_test = train_test_split(df_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4069f6b4-182c-4d72-89f1-8b16da440b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare the first n examples from df_train into a prompt that will be returned to the caller\n",
    "# The returned prompt will be used for few-shot training\n",
    "\n",
    "def prepare(n):\n",
    "    df_train_first_n_rows = df_train.iloc[0:n]\n",
    "    \n",
    "    prepared_text = []\n",
    "    \n",
    "    for _, row in df_train_first_n_rows.iterrows():\n",
    "        text = row['document']\n",
    "        label = str(row['RealNews?'])\n",
    "        concatenated_text = \" => \".join([text, label])\n",
    "        prepared_text.append(concatenated_text)\n",
    "\n",
    "    return \"\\n\".join(prepared_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b1dae90-098d-4fd1-b06e-a67de5437527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hillary lies: remember when hillary disclosed she was named after a famous person? this story is from 2006 and is the first in a long series of hillary s lies that we ll be exposing over the next couple of months for more than a decade, one piece of senator hillary rodham clinton s informal biography has been that she was named for sir edmund hillary, the conqueror of mount everest. the story was even recounted in bill clinton s autobiography.but yesterday, mrs. clinton s campaign said she was not named for sir edmund after all. it was a sweet family story her mother shared to inspire greatness in her daughter, to great results i might add,  said jennifer hanley, a spokeswoman for the campaign.in may 1953, sir edmund and his sherpa guide, tenzing norgay, became the first men to reach the summit of mount everest. in 1995, shortly after meeting sir edmund, mrs. clinton said that her mother, dorothy rodham, had long told her she was named for the famous mountaineer.  it had two l s, which is how she thought she was supposed to spell hillary,  mrs. clinton said at the time, after meeting sir edmund.  so when i was born, she called me hillary, and she always told me it s because of sir edmund hillary. even though bill clinton repeated the story in his 2004 autobiography,  my life,  hillary clinton did not mention it in her own autobiography,  living history,  which was published in 2003.but one big hole has been poked in the story over the years, both in cyberspace and elsewhere: sir edmund became famous only after climbing everest in 1953. mrs. clinton, as it happens, was born in 1947. via: nyt => False\n",
      "eu leaders likely to give go-ahead to new phase of brexit talks paris (reuters) - the leaders of the european union s remaining 27 member states are very likely to approve this week the deal struck by their chief negotiator with britain and move to a second phase of exit talks, a french presidency source said on wednesday. eu leaders are almost certain to judge on friday that  sufficient progress  has been made on the rights of citizens, the brexit divorce bill and the irish border to allow negotiations to move to the next phase. the eu executive recommended last week that leaders approve the start of trade talks.  => True\n"
     ]
    }
   ],
   "source": [
    "# Calling the prepare function with a small value of n (to verify it works properly) \n",
    "\n",
    "print(prepare(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2c984da-6999-4770-91fd-ad20da2296d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to append a row from validation to the prompt and return the complete prompt to the caller\n",
    "\n",
    "def generate_prompt(n, validation_row):\n",
    "    prompt = prepare(n)\n",
    "    text = validation_row['document'].iloc[0]\n",
    "    text = \" => \".join([text, ''])\n",
    "    return \"\\n\".join([prompt, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d69a93d9-5f82-47fe-bfd5-644a2f2cc298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hillary lies: remember when hillary disclosed she was named after a famous person? this story is from 2006 and is the first in a long series of hillary s lies that we ll be exposing over the next couple of months for more than a decade, one piece of senator hillary rodham clinton s informal biography has been that she was named for sir edmund hillary, the conqueror of mount everest. the story was even recounted in bill clinton s autobiography.but yesterday, mrs. clinton s campaign said she was not named for sir edmund after all. it was a sweet family story her mother shared to inspire greatness in her daughter, to great results i might add,  said jennifer hanley, a spokeswoman for the campaign.in may 1953, sir edmund and his sherpa guide, tenzing norgay, became the first men to reach the summit of mount everest. in 1995, shortly after meeting sir edmund, mrs. clinton said that her mother, dorothy rodham, had long told her she was named for the famous mountaineer.  it had two l s, which is how she thought she was supposed to spell hillary,  mrs. clinton said at the time, after meeting sir edmund.  so when i was born, she called me hillary, and she always told me it s because of sir edmund hillary. even though bill clinton repeated the story in his 2004 autobiography,  my life,  hillary clinton did not mention it in her own autobiography,  living history,  which was published in 2003.but one big hole has been poked in the story over the years, both in cyberspace and elsewhere: sir edmund became famous only after climbing everest in 1953. mrs. clinton, as it happens, was born in 1947. via: nyt => False\n",
      "eu leaders likely to give go-ahead to new phase of brexit talks paris (reuters) - the leaders of the european union s remaining 27 member states are very likely to approve this week the deal struck by their chief negotiator with britain and move to a second phase of exit talks, a french presidency source said on wednesday. eu leaders are almost certain to judge on friday that  sufficient progress  has been made on the rights of citizens, the brexit divorce bill and the irish border to allow negotiations to move to the next phase. the eu executive recommended last week that leaders approve the start of trade talks.  => True\n",
      "florida gov. scott not endorsing 2016 republican presidential candidate washington (reuters) - florida governor rick scott said on thursday he would not endorse anyone in the 2016 republican presidential race, despite the candidacy of a senator from his home state, marco rubio. “i trust the voters, so i will not try to tell the republican voters in florida how to vote by endorsing a candidate before our primary on march 15. i believed in the voters when i first ran for office, and i still believe in them today,” scott, a republican, said in a statement.  (reporting by ginger gibson; writing by doina chiacu; editing by mohammad zargham) this article was funded in part by sap. it was independently created by the reuters editorial staff. sap had no editorial involvement in its creation or production. => \n"
     ]
    }
   ],
   "source": [
    "# Calling the generate_prompt function to ensure it works properly\n",
    "\n",
    "n = 2\n",
    "validation_row = df_validation.sample(n=1)\n",
    "\n",
    "prompt = generate_prompt(n, validation_row)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc19138d-aad7-4137-bff4-52eff2ea4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained model and its corresponding tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98f168d8-fd2b-4ac5-a31c-2bbea2e20e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which passes the prompt to the pretrained model, gpt2, and returns the corresponding prediction\n",
    "\n",
    "def generate_label(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    \n",
    "    true_token_id = tokenizer.encode(\"True\", add_special_tokens=False)[0]\n",
    "    false_token_id = tokenizer.encode(\"False\", add_special_tokens=False)[0]\n",
    "    \n",
    "    true_logit = last_token_logits[0, true_token_id].item()\n",
    "    false_logit = last_token_logits[0, false_token_id].item()\n",
    "    \n",
    "    predicted_label = \"True\" if true_logit >= false_logit else \"False\"\n",
    "    return predicted_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5ede5556-4286-4023-8545-6ac22711b307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the generate_label function to ensure it works properly\n",
    "\n",
    "generate_label(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b03e8356-cd84-4cd6-85a0-5ca4b6782ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the F1 score on df_test\n",
    "# Compares the predicted label to the ground truth (gold label)\n",
    "\n",
    "def score(n, df_test):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    \n",
    "    for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing Rows\"):\n",
    "        row = row.to_frame().T\n",
    "        \n",
    "        prompt = generate_prompt(n, row)\n",
    "        predicted_label = generate_label(prompt)\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    y_true = df_test['RealNews?'].astype(\"string\").tolist()\n",
    "    y_pred = predicted_labels\n",
    "\n",
    "    f1_result = f1_score(y_true, y_pred, pos_label=\"True\")\n",
    "\n",
    "    return f1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3b1e8135-44ba-494d-8445-15e230a1b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of the rows from df_validation to use in the score function\n",
    "\n",
    "df_validation_sample = df_validation.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "af6a0b85-496e-48e1-831b-19ea71fbd018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.47863247863247865 when n equals 1 on df_validation_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.5588235294117647 when n equals 2 on df_validation_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.6111111111111112 when n equals 3 on df_validation_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 100/100 [01:30<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.6111111111111112 when n equals 4 on df_validation_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 100/100 [01:27<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.6111111111111112 when n equals 5 on df_validation_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the F1 score for n rangining between 1 and 5 inclusive to analyze differences in performance\n",
    "\n",
    "for n in range(1, 6):\n",
    "    f1_result = score(n, df_validation_sample)\n",
    "    print(f\"F1 score : {f1_result} when n equals {n} on df_validation_sample\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd324eeb-bb61-4aa1-9bc6-c3b7bb0f763d",
   "metadata": {},
   "source": [
    "As shown above, n iterates between 1 and 5 inclusive. As n increases from 1 to 3, the F1 score increases and afterwards stabilizes at around 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3f5cd434-d3e6-41e2-ac93-e0bb74ed47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of the rows from df_test to use in the score function\n",
    "\n",
    "df_test_sample = df_test.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d1dd51a4-3ea1-4cb4-ab6a-80eaf0d0f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 100/100 [01:26<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.6754966887417219 when n equals 3 on df_test_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calling the score function on df_test_sample\n",
    "\n",
    "n = 3\n",
    "\n",
    "f1_result = score(n, df_test_sample)\n",
    "print(f\"F1 score : {f1_result} when n equals {n} on df_test_sample\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce259244-a8db-4992-b655-b4261dc11ee5",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8330514-f9f8-4b6f-8f91-9c745a8ecc4c",
   "metadata": {},
   "source": [
    "I chose ChatGPT (GPT-4) to explore coding questions and evaluate its ability to write code. I tested prompts with common leeetcode coding questions. In terms of strengths, ChatGPT generates clean and readable code which is logically sound. It does well at well-defined tasks such as recursion and dynamic programming. In terms of weaknesses, vague prompts produced generic outputs. Overall, defining clear and precise prompts worked best and produced strong results.        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3f3c3ec-97cb-40be-840a-387bdc378fa8",
   "metadata": {},
   "source": [
    "I chose ChatGPT (GPT-4) to generate a mathematical proof, a story, and a cover letter. It excelled at creating coherent and grammatically correct text, especially for the story and cover letter. On the other hand, the mathematical proof was logical but lacked rigor for more complex proofs. In terms of strengths, ChatGPT was great at creative writing and structured cover letters. In terms of weaknesses, it struggled with more detailed proofs that need domain expertise. A language model may be better at some things than others because it depends on the prompt given. Generally, a more clear and precise prompt tends to yield better results.              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
